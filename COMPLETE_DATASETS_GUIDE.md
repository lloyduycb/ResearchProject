# Complete Guide: All C-MAPSS Datasets (FD001-FD004)

## ğŸ“š Dataset Overview

### FD001 - Simple Baseline â˜…â˜†â˜†
- **Operating Conditions**: Single (Sea level)
- **Fault Modes**: Single (HPC Degradation)
- **Training Units**: 100 engines
- **Test Units**: 100 engines
- **Use Case**: Baseline performance, algorithm development
- **Complexity**: LOW

### FD002 - Multiple Conditions â˜…â˜…â˜†
- **Operating Conditions**: Six different flight conditions
- **Fault Modes**: Single (HPC Degradation)
- **Training Units**: 260 engines
- **Test Units**: 259 engines
- **Use Case**: Test adaptability to operating condition variations
- **Complexity**: MEDIUM

### FD003 - Multiple Faults â˜…â˜…â˜†
- **Operating Conditions**: Single (Sea level)
- **Fault Modes**: Two (HPC + Fan Degradation)
- **Training Units**: 100 engines
- **Test Units**: 100 engines
- **Use Case**: Test ability to distinguish between fault types
- **Complexity**: MEDIUM

### FD004 - Maximum Complexity â˜…â˜…â˜…
- **Operating Conditions**: Six different flight conditions
- **Fault Modes**: Two (HPC + Fan Degradation)
- **Training Units**: 248 engines
- **Test Units**: 249 engines
- **Use Case**: Real-world scenario with maximum variability
- **Complexity**: HIGH

---

## ğŸ¯ How to Use This Toolkit with All Datasets

### Option 1: Analyze One Dataset at a Time

```python
# For FD001 (simplest)
python run_evaluation.py  # default is FD001

# For FD003 (multiple faults)
# Edit run_evaluation.py: change fd_number=1 to fd_number=3
python run_evaluation.py
```

### Option 2: Analyze All Datasets at Once (RECOMMENDED)

```bash
python run_all_datasets.py
```

This will:
1. âœ… Analyze all four datasets (FD001-FD004)
2. âœ… Generate individual results for each
3. âœ… Create cross-dataset comparisons
4. âœ… Analyze complexity impact
5. âœ… Provide overall model rankings

---

## ğŸ“Š Expected Results Pattern

### Typical RMSE Progression (Lower is Better)

```
Dataset    SVR      LSTM     TCN      Transformer
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
FD001     18-22    14-17    12-15    13-16      â† Easiest
FD002     22-28    18-24    16-20    17-21
FD003     20-25    16-20    14-18    15-19
FD004     24-32    20-28    18-24    19-25      â† Hardest
```

**Key Pattern**: 
- RMSE increases with complexity (FD001 â†’ FD004)
- Deep models (LSTM/TCN/Transformer) handle complexity better than SVR
- TCN typically shows best balance of accuracy and robustness

---

## ğŸ”¬ What Makes Each Dataset Important

### FD001: Validation of Core Algorithm
- **Purpose**: Prove your model works in ideal conditions
- **Paper Section**: "5.1 Performance on FD001 (Clean Data)"
- **What to Show**: Absolute best performance numbers
- **Key Statement**: "Under single operating condition, TCN achieved RMSE of X.XX..."

### FD002: Operating Condition Robustness
- **Purpose**: Show model adapts to different flight regimes
- **Paper Section**: "5.2 Performance on Complex Data"
- **What to Show**: Comparison with FD001, degradation analysis
- **Key Statement**: "When varying operating conditions, model maintained Y% of baseline performance..."

### FD003: Fault Discrimination Capability
- **Purpose**: Prove model distinguishes between fault types
- **Paper Section**: "5.2 Performance on Complex Data" or separate subsection
- **What to Show**: Similar to FD001 but with fault mode discussion
- **Key Statement**: "With multiple fault modes, model demonstrated ability to..."

### FD004: Real-World Validation
- **Purpose**: Most realistic scenario - proves deployment readiness
- **Paper Section**: "6. Discussion"
- **What to Show**: Comprehensive analysis, deployment recommendation
- **Key Statement**: "Under realistic operating conditions with multiple fault modes (FD004), the recommended TCN model achieved..."

---

## ğŸ“ Recommended Paper Structure Using All Datasets

### 5. Results and Analysis

#### 5.1 Baseline Performance (FD001)
- Present main comparison table with all models
- Include prediction vs actual plot
- Include error distribution analysis
- Show statistical significance

**Figure 1**: Model comparison on FD001  
**Table 1**: Performance metrics (FD001)  
**Table 2**: Statistical significance tests  

#### 5.2 Performance on Complex Data (FD003 or FD004)
- Show how performance changes with complexity
- Emphasize which models degrade gracefully
- Include robustness analysis

**Figure 2**: Model comparison on FD003/FD004  
**Table 3**: Performance metrics (FD003/FD004)  

#### 5.3 Cross-Dataset Analysis
- Show performance progression FD001â†’FD004
- Analyze generalization capability
- Overall model rankings

**Figure 3**: Performance across all datasets  
**Figure 4**: Complexity impact analysis  
**Table 4**: Cross-dataset summary  

### 6. Discussion and Critical Thinking

#### 6.1 Robustness to Sensor Noise (ALL DATASETS)
- Show robustness scores across all datasets
- Analyze consistency of robustness
- Cyber-security implications

**Figure 5**: Robustness comparison across datasets  
**Table 5**: Robustness summary  

#### 6.2 Implications for Deployment
- Overall recommendation based on all datasets
- Discuss trade-offs (accuracy vs robustness vs complexity)
- Deployment considerations

---

## ğŸ¯ Directory Structure for All Datasets

```
project/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ train_FD001.txt
â”‚   â”œâ”€â”€ test_FD001.txt
â”‚   â”œâ”€â”€ RUL_FD001.txt
â”‚   â”œâ”€â”€ train_FD002.txt
â”‚   â”œâ”€â”€ test_FD002.txt
â”‚   â”œâ”€â”€ RUL_FD002.txt
â”‚   â”œâ”€â”€ train_FD003.txt
â”‚   â”œâ”€â”€ test_FD003.txt
â”‚   â”œâ”€â”€ RUL_FD003.txt
â”‚   â”œâ”€â”€ train_FD004.txt
â”‚   â”œâ”€â”€ test_FD004.txt
â”‚   â””â”€â”€ RUL_FD004.txt
â”‚
â”œâ”€â”€ predictions/
â”‚   â”œâ”€â”€ SVR_FD001_predictions.npy
â”‚   â”œâ”€â”€ LSTM_FD001_predictions.npy
â”‚   â”œâ”€â”€ TCN_FD001_predictions.npy
â”‚   â”œâ”€â”€ Transformer_FD001_predictions.npy
â”‚   â”œâ”€â”€ SVR_FD002_predictions.npy
â”‚   â”œâ”€â”€ ... (repeat for FD002, FD003, FD004)
â”‚
â””â”€â”€ results/
    â”œâ”€â”€ FD001_results.csv
    â”œâ”€â”€ FD002_results.csv
    â”œâ”€â”€ FD003_results.csv
    â”œâ”€â”€ FD004_results.csv
    â”œâ”€â”€ all_datasets_rmse_summary.csv
    â”œâ”€â”€ all_datasets_robustness_summary.csv
    â”œâ”€â”€ model_rankings.csv
    â””â”€â”€ *.png (all visualizations)
```

---

## ğŸ’¡ Pro Tips for Each Dataset

### FD001 Tips
âœ… Use this for initial algorithm validation  
âœ… Get your best absolute numbers here  
âœ… Perfect for detailed error analysis  
âœ… Use for statistical significance tests  

### FD002 Tips
âœ… Emphasize operating condition adaptability  
âœ… Compare preprocessing strategies  
âœ… Discuss feature importance across conditions  
âœ… Show your model isn't overfitting to single condition  

### FD003 Tips
âœ… Emphasize fault discrimination capability  
âœ… Analyze which fault mode is harder to predict  
âœ… Discuss feature patterns for different faults  
âœ… Show model distinguishes between degradation types  

### FD004 Tips
âœ… This is your "real-world" validation  
âœ… Emphasize generalization capability  
âœ… Use for deployment recommendations  
âœ… Show robust performance under maximum complexity  

---

## ğŸ“Š Key Metrics to Report for Each Dataset

### Must Report for ALL Datasets
1. RMSE (primary metric)
2. MAE (interpretability)
3. RÂ² (goodness of fit)
4. PHM Score (asymmetric penalty)

### Report Once (Usually FD001)
1. Statistical significance tests
2. Detailed error distribution
3. Confidence intervals
4. Effect sizes

### Report in Cross-Dataset Analysis
1. Average RMSE across all datasets
2. Performance degradation (FD001 â†’ FD004)
3. Robustness consistency
4. Overall rankings

---

## ğŸ“ Sample Paper Statements Using All Datasets

### Introduction
```
"We evaluate our models across all four C-MAPSS sub-datasets (FD001-FD004), 
representing increasing levels of operational complexity, from single 
operating condition with single fault mode (FD001) to multiple operating 
conditions with multiple fault modes (FD004)."
```

### Methodology
```
"To ensure comprehensive evaluation, we tested each model on all four 
C-MAPSS datasets, varying in complexity from the baseline FD001 to the 
most challenging FD004 scenario."
```

### Results
```
"On the baseline FD001 dataset, TCN achieved an RMSE of X.XX (95% CI: 
Y.YY-Z.ZZ), significantly outperforming the SVR baseline (p < 0.001, 
d = 0.85, large effect). When evaluated on the most complex FD004 
dataset, TCN maintained robust performance (RMSE: A.AA), demonstrating 
only a B.B% degradation compared to FD001."
```

### Discussion
```
"Cross-dataset analysis revealed that TCN exhibited the most consistent 
performance across all operating conditions and fault modes, with an 
average RMSE of X.XX across FD001-FD004. This generalization capability, 
combined with superior robustness scores (avg: 91.2/100), makes TCN the 
recommended architecture for deployment in real-world aviation maintenance 
systems."
```

---

## âœ… Checklist for Complete Analysis

### Data Preparation
- [ ] Downloaded all C-MAPSS datasets (FD001-FD004)
- [ ] Placed in correct directory structure
- [ ] Verified file formats and contents

### Model Training
- [ ] Trained models on FD001 training set
- [ ] Trained models on FD002 training set (if evaluating FD002)
- [ ] Trained models on FD003 training set
- [ ] Trained models on FD004 training set (if evaluating FD004)

### Prediction Generation
- [ ] Generated predictions for all models on FD001 test
- [ ] Generated predictions for all models on FD003 test
- [ ] (Optional) Generated for FD002 and FD004
- [ ] Saved in correct format (.npy files)

### Analysis Execution
- [ ] Ran individual dataset evaluations
- [ ] Ran cross-dataset comparison
- [ ] Generated all visualizations
- [ ] Created summary tables

### Paper Integration
- [ ] Selected key figures (4-6 figures max)
- [ ] Selected key tables (3-5 tables max)
- [ ] Wrote results section with statistical support
- [ ] Emphasized cross-dataset generalization
- [ ] Discussed robustness implications

---

## ğŸš€ Quick Start Commands

### Minimum Analysis (FD001 + FD003)
```bash
# Analyze simplest and multi-fault datasets
python run_evaluation.py  # FD001 by default

# Edit run_evaluation.py: fd_number=3
python run_evaluation.py  # FD003
```

### Comprehensive Analysis (All Datasets)
```bash
# One command to rule them all!
python run_all_datasets.py
```

### Just Generate Demo Results
```bash
# See what outputs look like
python demo.py
```

---

## ğŸ“ˆ Expected Timeline

### Minimum Viable Paper (FD001 only)
- Data prep: 1 hour
- Training: 2-4 hours
- Analysis: 15 minutes
- **Total: ~1 day**

### Good Paper (FD001 + FD003)
- Data prep: 1 hour
- Training: 4-8 hours
- Analysis: 30 minutes
- **Total: ~1-2 days**

### Excellent Paper (All FD001-FD004)
- Data prep: 2 hours
- Training: 8-16 hours
- Analysis: 1 hour
- **Total: ~2-3 days**

---

## ğŸ¯ Bottom Line

**Minimum for passing**: FD001 only  
**Recommended for good grade**: FD001 + FD003  
**Recommended for excellent grade**: All datasets with cross-analysis  

The toolkit supports all approaches - choose based on your:
- Time available
- Computational resources
- Desired paper depth
- Target grade

**Good luck!** ğŸš€
