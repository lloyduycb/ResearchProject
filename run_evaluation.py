"""
C-MAPSS Data Loader and Model Evaluation Script
================================================
This script loads NASA C-MAPSS dataset and evaluates your trained models.

Usage:
1. Place your trained models' predictions in the correct format
2. Run this script to generate comprehensive analysis
3. All outputs saved to ./results directory
"""

import numpy as np
import pandas as pd
from rul_analysis_toolkit import RULAnalyzer, VisualizationSuite, ComprehensiveReport
import os
import pickle
from typing import Dict, Tuple


class CMAPSSDataLoader:
    """
    Load and preprocess C-MAPSS dataset for evaluation.
    """
    
    def __init__(self, data_path: str = './data'):
        self.data_path = data_path
        self.train_df = None
        self.test_df = None
        self.rul_df = None
        
    def load_fd_dataset(self, fd_number: int) -> Tuple[pd.DataFrame, pd.DataFrame, np.ndarray]:
        """
        Load specific FD dataset (FD001, FD002, FD003, or FD004).
        
        Returns:
            train_df: Training data
            test_df: Test data  
            true_rul: True RUL values for test set
        """
        # Column names
        index_names = ['unit_id', 'time_cycles']
        setting_names = ['setting_1', 'setting_2', 'setting_3']
        sensor_names = [f'sensor_{i}' for i in range(1, 22)]
        col_names = index_names + setting_names + sensor_names
        
        # Load training data
        train_path = f'{self.data_path}/train_FD00{fd_number}.txt'
        train_df = pd.read_csv(train_path, sep=r'\s+', header=None, names=col_names)
        
        # Load test data
        test_path = f'{self.data_path}/test_FD00{fd_number}.txt'
        test_df = pd.read_csv(test_path, sep=r'\s+', header=None, names=col_names)
        
        # Load RUL truth
        rul_path = f'{self.data_path}/RUL_FD00{fd_number}.txt'
        true_rul = pd.read_csv(rul_path, sep=r'\s+', header=None).values.flatten()
        
        print(f"[OK] Loaded FD00{fd_number}:")
        print(f"  Training samples: {len(train_df)}")
        print(f"  Test engines: {test_df['unit_id'].nunique()}")
        print(f"  Test samples: {len(test_df)}")
        
        return train_df, test_df, true_rul
    
    def add_rul_column(self, df: pd.DataFrame, cap: int = 125) -> pd.DataFrame:
        """
        Add RUL column with piecewise linear capping.
        """
        # Calculate RUL for each engine
        rul_list = []
        for unit_id in df['unit_id'].unique():
            unit_data = df[df['unit_id'] == unit_id]
            max_cycle = unit_data['time_cycles'].max()
            unit_rul = max_cycle - unit_data['time_cycles']
            unit_rul = np.minimum(unit_rul, cap)  # Cap at 125
            rul_list.extend(unit_rul.values)
        
        df['RUL'] = rul_list
        return df
    
    def remove_constant_sensors(self, train_df: pd.DataFrame, test_df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]:
        """
        Remove sensors with zero variance.
        """
        sensor_cols = [col for col in train_df.columns if col.startswith('sensor_')]
        
        # Calculate variance on training set
        variances = train_df[sensor_cols].var()
        constant_sensors = variances[variances == 0].index.tolist()
        
        print(f"  Removing {len(constant_sensors)} constant sensors: {constant_sensors}")
        
        train_df = train_df.drop(columns=constant_sensors)
        test_df = test_df.drop(columns=constant_sensors)
        
        return train_df, test_df


class ModelPredictionLoader:
    """
    Load model predictions from various formats.
    """
    
    @staticmethod
    def load_from_numpy(filepath: str) -> np.ndarray:
        """Load predictions from .npy file."""
        return np.load(filepath)
    
    @staticmethod
    def load_from_pickle(filepath: str) -> np.ndarray:
        """Load predictions from pickle file."""
        with open(filepath, 'rb') as f:
            return pickle.load(f)
    
    @staticmethod
    def load_from_csv(filepath: str, column: str = 'prediction') -> np.ndarray:
        """Load predictions from CSV file."""
        df = pd.read_csv(filepath)
        return df[column].values
    
    @staticmethod
    def generate_test_predictions(y_true: np.ndarray, model_name: str, 
                                  noise_level: float = 5.0) -> np.ndarray:
        """
        Generate synthetic predictions for testing (REPLACE WITH ACTUAL PREDICTIONS).
        """
        np.random.seed(hash(model_name) % 2**32)
        
        # Different noise levels for different model types
        noise_factors = {
            'SVR': 10.0,
            'LSTM': 7.0,
            'TCN': 5.0,
            'Transformer': 5.5
        }
        
        noise = noise_factors.get(model_name, noise_level)
        predictions = y_true + np.random.normal(0, noise, len(y_true))
        
        # Ensure non-negative predictions
        predictions = np.maximum(predictions, 0)
        
        return predictions


def run_comprehensive_evaluation(fd_number: int = 1, 
                                 data_path: str = './data',
                                 predictions_path: str = './predictions'):
    """
    Main evaluation pipeline for C-MAPSS dataset.
    
    Args:
        fd_number: Which FD dataset to evaluate (1-4)
        data_path: Path to C-MAPSS data files
        predictions_path: Path to model prediction files
    """
    
    print("\n" + "="*70)
    print(f"COMPREHENSIVE RUL EVALUATION - FD00{fd_number}")
    print("="*70)
    
    # Initialize components
    loader = CMAPSSDataLoader(data_path)
    pred_loader = ModelPredictionLoader()
    report = ComprehensiveReport('./results')
    viz = VisualizationSuite()
    
    # Load data
    print("\n[1/6] Loading C-MAPSS dataset...")
    try:
        train_df, test_df, true_rul = loader.load_fd_dataset(fd_number)
        train_df = loader.add_rul_column(train_df)
        train_df, test_df = loader.remove_constant_sensors(train_df, test_df)
    except FileNotFoundError:
        print(f"\n[WARNING] C-MAPSS data not found at {data_path}")
        print("Generating synthetic data for demonstration...")
        true_rul = np.random.exponential(50, 100)
    
    # Load model predictions
    print("\n[2/6] Loading model predictions...")
    models = ['SVR', 'LSTM', 'TCN', 'Transformer']
    predictions = {}
    
    for model_name in models:
        pred_file = f'{predictions_path}/{model_name}_FD00{fd_number}_predictions.npy'
        
        if os.path.exists(pred_file):
            predictions[model_name] = pred_loader.load_from_numpy(pred_file)
            print(f"  [OK] Loaded {model_name} predictions from {pred_file}")
        else:
            print(f"  [WARNING] {model_name} predictions not found, generating synthetic data...")
            predictions[model_name] = pred_loader.generate_test_predictions(true_rul, model_name)
    
    # Generate noisy predictions for robustness testing
    print("\n[3/6] Generating noisy predictions for robustness analysis...")
    noise_level = 0.01  # 1% Gaussian noise
    noisy_predictions = {}
    
    for model_name, clean_pred in predictions.items():
        # Add Gaussian noise to test data (simulating sensor degradation)
        noise = np.random.normal(0, noise_level * np.std(clean_pred), len(clean_pred))
        noisy_predictions[model_name] = clean_pred + noise
    
    # Run analysis for each model
    print("\n[4/6] Computing metrics and statistics...")
    all_results = {}
    robustness_results = {}
    
    for model_name, y_pred in predictions.items():
        print(f"\n  Analyzing {model_name}...")
        analyzer = RULAnalyzer(model_name)
        
        # Standard metrics
        metrics = analyzer.compute_standard_metrics(true_rul, y_pred)
        
        # Confidence intervals
        ci = analyzer.compute_confidence_intervals(true_rul, y_pred)
        metrics['RMSE_CI_Lower'] = ci['RMSE_CI'][0]
        metrics['RMSE_CI_Upper'] = ci['RMSE_CI'][1]
        
        # Error distribution
        error_stats = analyzer.analyze_error_distribution(true_rul, y_pred)
        metrics.update(error_stats)
        
        # Lifecycle performance
        lifecycle = analyzer.early_vs_late_performance(true_rul, y_pred)
        metrics.update(lifecycle)
        
        # Robustness analysis (CRITICAL for cyber-security)
        robustness = analyzer.robustness_to_noise(
            true_rul, y_pred, noisy_predictions[model_name], noise_level
        )
        
        all_results[model_name] = metrics
        robustness_results[model_name] = robustness
        
        print(f"    RMSE: {metrics['RMSE']:.2f} Â± {(ci['RMSE_CI'][1] - ci['RMSE_CI'][0])/2:.2f}")
        print(f"    Robustness Score: {robustness['Robustness_Score']:.1f}/100")
    
    # Generate visualizations
    print("\n[5/6] Generating publication-quality visualizations...")
    
    for model_name, y_pred in predictions.items():
        viz.plot_prediction_vs_actual(
            true_rul, y_pred, model_name,
            f'./results/FD00{fd_number}_{model_name}_prediction_vs_actual.png'
        )
        
        viz.plot_error_distribution(
            true_rul, y_pred, model_name,
            f'./results/FD00{fd_number}_{model_name}_error_distribution.png'
        )
    
    # Comparative plots
    viz.plot_robustness_comparison(
        robustness_results,
        f'./results/FD00{fd_number}_robustness_comparison.png'
    )
    
    viz.plot_lifecycle_performance(
        true_rul, predictions,
        f'./results/FD00{fd_number}_lifecycle_performance.png'
    )
    
    # Generate report tables
    print("\n[6/6] Generating summary tables...")
    
    # Main results table
    results_df = report.generate_comparison_table(all_results, latex_format=False)
    print("\n" + "="*70)
    print("PERFORMANCE COMPARISON TABLE")
    print("="*70)
    print(results_df[['RMSE', 'MAE', 'R2', 'PHM_Score', 'Early_Life_RMSE', 'Late_Life_RMSE']])
    
    # Export LaTeX version with formatting
    results_df_latex = report.generate_comparison_table(all_results, latex_format=True)
    report.export_latex_table(results_df_latex, f'FD00{fd_number}_results_table.tex')
    
    # Save CSV without LaTeX formatting
    results_df.to_csv(f'./results/FD00{fd_number}_results.csv')
    
    # Robustness table
    robustness_df = pd.DataFrame(robustness_results).T
    print("\n" + "="*70)
    print("ROBUSTNESS ANALYSIS (Cyber-Security Assessment)")
    print("="*70)
    print(robustness_df[['Clean_RMSE', 'Noisy_RMSE', 'Degradation_Percent', 'Robustness_Score']])
    
    robustness_df.to_csv(f'./results/FD00{fd_number}_robustness.csv')
    
    # Generate key insights
    print("\n" + "="*70)
    print("KEY INSIGHTS")
    print("="*70)
    
    # Best performing model
    best_model = results_df['RMSE'].astype(float).idxmin()
    print(f"\n[OK] Best Overall Performance: {best_model}")
    print(f"  RMSE: {results_df.loc[best_model, 'RMSE']}")
    print(f"  MAE: {results_df.loc[best_model, 'MAE']}")
    
    # Most robust model
    most_robust = robustness_df['Robustness_Score'].astype(float).idxmax()
    print(f"\n[OK] Most Robust Model (Cyber-Security): {most_robust}")
    print(f"  Robustness Score: {robustness_df.loc[most_robust, 'Robustness_Score']:.1f}/100")
    print(f"  Performance Degradation: {robustness_df.loc[most_robust, 'Degradation_Percent']:.1f}%")
    
    # Critical phase performance
    critical_performance = {model: all_results[model]['Late_Life_RMSE'] 
                           for model in models}
    best_critical = min(critical_performance, key=critical_performance.get)
    print(f"\n[OK] Best Critical Phase Performance (<50 cycles): {best_critical}")
    print(f"  Late Life RMSE: {critical_performance[best_critical]:.2f}")
    
    # Recommendation
    print("\n" + "="*70)
    print("DEPLOYMENT RECOMMENDATION")
    print("="*70)
    
    # Calculate composite score (accuracy + robustness)
    composite_scores = {}
    for model in models:
        accuracy_score = 100 - (results_df.loc[model, 'RMSE'] / results_df['RMSE'].astype(float).max() * 100)
        robustness_score = robustness_df.loc[model, 'Robustness_Score']
        composite_scores[model] = 0.6 * accuracy_score + 0.4 * robustness_score
    
    recommended_model = max(composite_scores, key=composite_scores.get)
    print(f"\n>>> RECOMMENDED MODEL: {recommended_model}")
    print(f"\nRationale:")
    print(f"  - Composite Score: {composite_scores[recommended_model]:.1f}/100")
    print(f"  - Accuracy (60%): {results_df.loc[recommended_model, 'RMSE']} RMSE")
    print(f"  - Robustness (40%): {robustness_df.loc[recommended_model, 'Robustness_Score']:.1f}/100")
    print(f"\nThis model provides the best balance of predictive accuracy and")
    print(f"resilience to sensor perturbations, making it suitable for deployment")
    print(f"in safety-critical aviation maintenance systems.")
    
    print("\n" + "="*70)
    print("[OK] EVALUATION COMPLETE")
    print("="*70)
    print(f"\nAll results saved to: ./results/")
    print(f"  - Visualizations: PNG files")
    print(f"  - Tables: CSV and LaTeX files")
    print(f"  - Ready for inclusion in your research paper!")
    
    return all_results, robustness_results


if __name__ == "__main__":
    # Create necessary directories
    os.makedirs('./results', exist_ok=True)
    os.makedirs('./data', exist_ok=True)
    os.makedirs('./predictions', exist_ok=True)
    
    print("\n" + "="*70)
    print("NASA C-MAPSS RUL EVALUATION SUITE")
    print("Research Project CYS6001-20")
    print("="*70)
    
    print("\n[FILES] Directory Structure:")
    print("  ./data/        - Place C-MAPSS dataset files here")
    print("  ./predictions/ - Place your model predictions here")
    print("  ./results/     - Output directory (auto-created)")
    
    print("\n" + "="*70)
    
    # Run evaluation
    # Change fd_number to evaluate different datasets (1, 2, 3, or 4)
    results, robustness = run_comprehensive_evaluation(
        fd_number=1,  # FD001 - simplest case
        data_path='./data',
        predictions_path='./predictions'
    )
    
    print("\n\n[TIP] NEXT STEPS:")
    print("="*70)
    print("1. Replace synthetic predictions with your actual model outputs")
    print("2. Save predictions as: ./predictions/[MODEL]_FD001_predictions.npy")
    print("3. Re-run this script to generate real results")
    print("4. Copy generated figures and tables into your paper")
    print("5. Repeat for FD003 to show performance on complex data")
    print("="*70)
