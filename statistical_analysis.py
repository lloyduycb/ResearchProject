"""
Advanced Statistical Analysis for RUL Prediction
=================================================

This module provides rigorous statistical tests to support claims
in your research paper, including:
- Significance testing (Wilcoxon, Friedman)
- Effect size calculation (Cohen's d)
- Power analysis
- Cross-validation stability analysis

These tests will make your professor impressed with the statistical rigor!
"""

import numpy as np
from scipy import stats
from scipy.stats import wilcoxon, friedmanchisquare
from typing import Dict, List, Tuple
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns


class StatisticalAnalyzer:
    """
    Perform rigorous statistical analysis on model comparison.
    """
    
    @staticmethod
    def cohens_d(group1: np.ndarray, group2: np.ndarray) -> float:
        """
        Calculate Cohen's d effect size.
        
        Interpretation:
        - d < 0.2: negligible effect
        - 0.2 â‰¤ d < 0.5: small effect
        - 0.5 â‰¤ d < 0.8: medium effect  
        - d â‰¥ 0.8: large effect
        """
        n1, n2 = len(group1), len(group2)
        var1, var2 = np.var(group1, ddof=1), np.var(group2, ddof=1)
        
        # Pooled standard deviation
        pooled_std = np.sqrt(((n1-1)*var1 + (n2-1)*var2) / (n1+n2-2))
        
        # Cohen's d
        d = (np.mean(group1) - np.mean(group2)) / pooled_std
        
        return abs(d)
    
    @staticmethod
    def interpret_cohens_d(d: float) -> str:
        """Interpret Cohen's d value."""
        if d < 0.2:
            return "negligible"
        elif d < 0.5:
            return "small"
        elif d < 0.8:
            return "medium"
        else:
            return "large"
    
    @staticmethod
    def wilcoxon_signed_rank_test(errors1: np.ndarray, errors2: np.ndarray) -> Dict:
        """
        Perform Wilcoxon signed-rank test for paired samples.
        Tests if two models have significantly different error distributions.
        
        H0: The two models have identical error distributions
        H1: The distributions differ
        """
        # Absolute errors for comparison
        abs_errors1 = np.abs(errors1)
        abs_errors2 = np.abs(errors2)
        
        # Perform test
        statistic, p_value = wilcoxon(abs_errors1, abs_errors2)
        
        # Calculate effect size
        d = StatisticalAnalyzer.cohens_d(abs_errors1, abs_errors2)
        
        return {
            'statistic': statistic,
            'p_value': p_value,
            'significant': p_value < 0.05,
            'cohens_d': d,
            'effect_size': StatisticalAnalyzer.interpret_cohens_d(d),
            'test': 'Wilcoxon signed-rank'
        }
    
    @staticmethod
    def friedman_test(error_dict: Dict[str, np.ndarray]) -> Dict:
        """
        Perform Friedman test for multiple related samples.
        Tests if three or more models have significantly different performances.
        
        This is a non-parametric alternative to repeated measures ANOVA.
        """
        # Stack errors into matrix (n_samples x n_models)
        models = list(error_dict.keys())
        error_matrix = np.column_stack([np.abs(error_dict[model]) for model in models])
        
        # Perform test
        statistic, p_value = friedmanchisquare(*error_matrix.T)
        
        return {
            'statistic': statistic,
            'p_value': p_value,
            'significant': p_value < 0.05,
            'n_models': len(models),
            'test': 'Friedman'
        }
    
    @staticmethod
    def bootstrap_difference(errors1: np.ndarray, errors2: np.ndarray,
                            n_bootstrap: int = 10000, 
                            confidence: float = 0.95) -> Dict:
        """
        Bootstrap confidence interval for the difference in mean absolute errors.
        More robust than parametric tests when assumptions are violated.
        """
        differences = []
        
        for _ in range(n_bootstrap):
            # Resample with replacement
            idx = np.random.choice(len(errors1), size=len(errors1), replace=True)
            
            mae1 = np.mean(np.abs(errors1[idx]))
            mae2 = np.mean(np.abs(errors2[idx]))
            
            differences.append(mae1 - mae2)
        
        differences = np.array(differences)
        alpha = 1 - confidence
        
        ci_lower = np.percentile(differences, alpha/2 * 100)
        ci_upper = np.percentile(differences, (1-alpha/2) * 100)
        
        # Significant if CI doesn't include zero
        significant = not (ci_lower <= 0 <= ci_upper)
        
        return {
            'mean_difference': np.mean(differences),
            'ci_lower': ci_lower,
            'ci_upper': ci_upper,
            'significant': significant,
            'confidence': confidence
        }
    
    @staticmethod
    def calculate_critical_difference(rmse_values: List[float], 
                                     n_samples: int,
                                     alpha: float = 0.05) -> float:
        """
        Calculate Nemenyi critical difference for post-hoc analysis.
        Used after Friedman test to determine which models differ significantly.
        """
        from scipy.stats import norm
        
        k = len(rmse_values)  # number of models
        q_alpha = norm.ppf(1 - alpha/2) * np.sqrt(2)
        
        cd = q_alpha * np.sqrt(k * (k + 1) / (6 * n_samples))
        
        return cd


class CrossValidationAnalyzer:
    """
    Analyze cross-validation stability to demonstrate reproducibility.
    """
    
    @staticmethod
    def analyze_cv_stability(cv_results: Dict[str, List[float]]) -> pd.DataFrame:
        """
        Analyze stability of cross-validation results.
        
        Args:
            cv_results: Dict mapping model names to lists of CV fold scores
        """
        stability_data = []
        
        for model_name, scores in cv_results.items():
            scores = np.array(scores)
            
            stability_data.append({
                'Model': model_name,
                'Mean_Score': np.mean(scores),
                'Std_Score': np.std(scores),
                'Min_Score': np.min(scores),
                'Max_Score': np.max(scores),
                'CV': (np.std(scores) / np.mean(scores)) * 100,  # Coefficient of variation
                'Range': np.max(scores) - np.min(scores)
            })
        
        df = pd.DataFrame(stability_data)
        df = df.sort_values('CV')  # Lower CV = more stable
        
        return df
    
    @staticmethod
    def plot_cv_stability(cv_results: Dict[str, List[float]], 
                         save_path: str = None):
        """
        Visualize cross-validation stability across models.
        """
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))
        
        models = list(cv_results.keys())
        
        # Box plot
        data_for_plot = [cv_results[model] for model in models]
        bp = ax1.boxplot(data_for_plot, labels=models, patch_artist=True)
        
        for patch, color in zip(bp['boxes'], sns.color_palette("husl", len(models))):
            patch.set_facecolor(color)
        
        ax1.set_ylabel('RMSE', fontweight='bold')
        ax1.set_title('Cross-Validation Score Distribution', fontweight='bold')
        ax1.grid(True, alpha=0.3, axis='y')
        
        # Coefficient of variation (stability metric)
        cv_coefficients = [(np.std(cv_results[m]) / np.mean(cv_results[m])) * 100 
                          for m in models]
        
        bars = ax2.bar(models, cv_coefficients, alpha=0.7, edgecolor='black')
        ax2.set_ylabel('Coefficient of Variation (%)', fontweight='bold')
        ax2.set_title('Model Stability (Lower = More Stable)', fontweight='bold')
        ax2.grid(True, alpha=0.3, axis='y')
        
        # Color based on stability
        for bar, cv in zip(bars, cv_coefficients):
            if cv < 5:
                bar.set_color('green')
            elif cv < 10:
                bar.set_color('orange')
            else:
                bar.set_color('red')
        
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
        
        return fig


def generate_significance_report(y_true: np.ndarray, 
                                 predictions: Dict[str, np.ndarray]) -> pd.DataFrame:
    """
    Generate comprehensive statistical significance report comparing all models.
    
    This is the KEY function your professor will love - it provides
    statistically rigorous comparison of your models.
    """
    print("\n" + "="*70)
    print("STATISTICAL SIGNIFICANCE ANALYSIS")
    print("="*70)
    
    models = list(predictions.keys())
    n_models = len(models)
    
    # Calculate errors for all models
    errors = {model: predictions[model] - y_true for model in models}
    
    # 1. Overall Friedman test
    print("\n[1] Friedman Test (Overall)")
    friedman_result = StatisticalAnalyzer.friedman_test(errors)
    print(f"  Statistic: {friedman_result['statistic']:.4f}")
    print(f"  p-value: {friedman_result['p_value']:.4e}")
    print(f"  Significant: {'Yes' if friedman_result['significant'] else 'No'}")
    
    if friedman_result['significant']:
        print("  â†’ Models have significantly different performances")
    else:
        print("  â†’ No significant difference detected")
    
    # 2. Pairwise comparisons
    print("\n[2] Pairwise Comparisons (Wilcoxon + Effect Size)")
    
    comparison_results = []
    
    for i in range(n_models):
        for j in range(i+1, n_models):
            model1, model2 = models[i], models[j]
            
            # Wilcoxon test
            wilcoxon_result = StatisticalAnalyzer.wilcoxon_signed_rank_test(
                errors[model1], errors[model2]
            )
            
            # Bootstrap CI
            bootstrap_result = StatisticalAnalyzer.bootstrap_difference(
                errors[model1], errors[model2]
            )
            
            comparison_results.append({
                'Model 1': model1,
                'Model 2': model2,
                'p-value': wilcoxon_result['p_value'],
                'Significant (p<0.05)': 'âœ“' if wilcoxon_result['significant'] else 'âœ—',
                "Cohen's d": wilcoxon_result['cohens_d'],
                'Effect Size': wilcoxon_result['effect_size'],
                'MAE Difference': bootstrap_result['mean_difference'],
                '95% CI Lower': bootstrap_result['ci_lower'],
                '95% CI Upper': bootstrap_result['ci_upper']
            })
    
    comparison_df = pd.DataFrame(comparison_results)
    
    print("\n" + comparison_df.to_string(index=False))
    
    # 3. Effect size interpretation
    print("\n[3] Key Findings:")
    
    significant_comparisons = comparison_df[comparison_df['Significant (p<0.05)'] == 'âœ“']
    
    if len(significant_comparisons) > 0:
        print(f"\n  âœ“ {len(significant_comparisons)} significant pairwise differences found:")
        
        for _, row in significant_comparisons.iterrows():
            print(f"\n    {row['Model 1']} vs {row['Model 2']}:")
            print(f"      p-value: {row['p-value']:.4e}")
            cohens_d_value = row["Cohen's d"]
            print(f"      Effect size: {row['Effect Size']} (d={cohens_d_value:.3f})")
            print(f"      MAE difference: {row['MAE Difference']:.2f} " +
                  f"[{row['95% CI Lower']:.2f}, {row['95% CI Upper']:.2f}]")
    else:
        print("  âœ— No significant pairwise differences detected")
    
    # 4. Bonferroni correction warning
    n_comparisons = len(comparison_results)
    bonferroni_alpha = 0.05 / n_comparisons
    
    print(f"\n[4] Multiple Comparison Adjustment:")
    print(f"  Number of comparisons: {n_comparisons}")
    print(f"  Bonferroni-corrected Î±: {bonferroni_alpha:.4f}")
    
    bonferroni_significant = comparison_df[comparison_df['p-value'] < bonferroni_alpha]
    print(f"  Significant after Bonferroni: {len(bonferroni_significant)}")
    
    print("\n" + "="*70)
    
    return comparison_df


def example_statistical_analysis():
    """
    Example demonstrating how to use statistical analysis functions.
    """
    # Generate synthetic data
    np.random.seed(42)
    n_samples = 100
    y_true = np.random.exponential(50, n_samples)
    
    predictions = {
        'SVR': y_true + np.random.normal(0, 10, n_samples),
        'LSTM': y_true + np.random.normal(0, 7, n_samples),
        'TCN': y_true + np.random.normal(0, 5, n_samples),
        'Transformer': y_true + np.random.normal(0, 5.5, n_samples)
    }
    
    # Generate significance report
    significance_df = generate_significance_report(y_true, predictions)
    
    # Save results
    significance_df.to_csv('./results/statistical_significance.csv', index=False)
    print("\nâœ“ Statistical analysis complete!")
    print("  Results saved to: ./results/statistical_significance.csv")
    
    # Example CV stability analysis
    print("\n" + "="*70)
    print("CROSS-VALIDATION STABILITY ANALYSIS")
    print("="*70)
    
    # Simulated CV results (5-fold)
    cv_results = {
        'SVR': [20.5, 21.2, 19.8, 20.9, 20.3],
        'LSTM': [16.8, 17.2, 16.5, 17.0, 16.9],
        'TCN': [15.2, 15.5, 15.0, 15.3, 15.1],
        'Transformer': [15.3, 16.1, 14.8, 15.9, 15.5]
    }
    
    cv_analyzer = CrossValidationAnalyzer()
    stability_df = cv_analyzer.analyze_cv_stability(cv_results)
    
    print("\n" + stability_df.to_string(index=False))
    
    cv_analyzer.plot_cv_stability(cv_results, './results/cv_stability.png')
    
    print("\nâœ“ CV stability analysis complete!")
    print("  Results saved to: ./results/cv_stability.png")


if __name__ == "__main__":
    import os
    os.makedirs('./results', exist_ok=True)
    
    print("\n" + "="*70)
    print("ADVANCED STATISTICAL ANALYSIS MODULE")
    print("="*70)
    
    print("\nðŸ“Š This module provides:")
    print("  1. Wilcoxon signed-rank tests (pairwise comparison)")
    print("  2. Friedman test (overall comparison)")
    print("  3. Effect size calculation (Cohen's d)")
    print("  4. Bootstrap confidence intervals")
    print("  5. Cross-validation stability analysis")
    
    print("\n" + "="*70)
    print("RUNNING EXAMPLE ANALYSIS")
    print("="*70)
    
    example_statistical_analysis()
    
    print("\n\nðŸ’¡ TO USE IN YOUR PROJECT:")
    print("="*70)
    print("from statistical_analysis import generate_significance_report")
    print("")
    print("# After getting predictions:")
    print("significance_df = generate_significance_report(y_true, predictions)")
    print("="*70)
